{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/livio/Desktop/Seminar/venv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\" Deep Neural Network for MNIST dataset classification task.\n",
    "References:\n",
    "    Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. \"Gradient-based\n",
    "    learning applied to document recognition.\" Proceedings of the IEEE,\n",
    "    86(11):2278-2324, November 1998.\n",
    "Links:\n",
    "    [MNIST Dataset] http://yann.lecun.com/exdb/mnist/\n",
    "\"\"\"\n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "from AMSGrad import *\n",
    "\n",
    "# Data loading and preprocessing\n",
    "import tflearn.datasets.mnist as mnist\n",
    "\n",
    "X, Y, testX, testY = mnist.load_data(one_hot=True)\n",
    "N_EPOCHS = 15\n",
    "\n",
    "def get_layers():\n",
    "    # Building deep neural network\n",
    "    input_layer = tflearn.input_data(shape=[None, 784])\n",
    "    dense1 = tflearn.fully_connected(input_layer, 64, activation='tanh',\n",
    "                                     regularizer='L2')\n",
    "    dense2 = tflearn.fully_connected(dense1, 64, activation='tanh',\n",
    "                                     regularizer='L2')\n",
    "    dense3 = tflearn.fully_connected(dense2, 64, activation='tanh',\n",
    "                                     regularizer='L2')\n",
    "    softmax = tflearn.fully_connected(dense3, 10, activation='softmax')\n",
    "    \n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 12899  | total loss: \u001b[1m\u001b[32m0.53668\u001b[0m\u001b[0m | time: 2.686s\n",
      "| SGD | epoch: 015 | loss: 0.53668 - acc: 0.8689 -- iter: 54976/55000\n",
      "Training Step: 12900  | total loss: \u001b[1m\u001b[32m0.51576\u001b[0m\u001b[0m | time: 3.756s\n",
      "| SGD | epoch: 015 | loss: 0.51576 - acc: 0.8774 | val_loss: 0.40393 - val_acc: 0.8898 -- iter: 55000/55000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# SGD\n",
    "tf.reset_default_graph()\n",
    "sgd = tflearn.SGD(learning_rate=0.01)\n",
    "net = tflearn.regression(get_layers(), optimizer=sgd,\n",
    "                         loss='categorical_crossentropy')\n",
    "\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(X, Y, n_epoch=N_EPOCHS, validation_set=(testX, testY),\n",
    "          show_metric=True, run_id=\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 12899  | total loss: \u001b[1m\u001b[32m0.06313\u001b[0m\u001b[0m | time: 2.295s\n",
      "| Optimizer | epoch: 015 | loss: 0.06313 - acc: 0.9770 -- iter: 54976/55000\n",
      "Training Step: 12900  | total loss: \u001b[1m\u001b[32m0.05888\u001b[0m\u001b[0m | time: 3.366s\n",
      "| Optimizer | epoch: 015 | loss: 0.05888 - acc: 0.9793 | val_loss: 0.09902 - val_acc: 0.9704 -- iter: 55000/55000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Momentum\n",
    "tf.reset_default_graph()\n",
    "momentum = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9)\n",
    "net = tflearn.regression(get_layers(), optimizer=momentum,\n",
    "                         loss='categorical_crossentropy')\n",
    "\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(X, Y, n_epoch=N_EPOCHS, validation_set=(testX, testY),\n",
    "          show_metric=True, run_id=\"momentum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 12899  | total loss: \u001b[1m\u001b[32m0.27147\u001b[0m\u001b[0m | time: 2.262s\n",
      "| Optimizer | epoch: 015 | loss: 0.27147 - acc: 0.9695 -- iter: 54976/55000\n",
      "Training Step: 12900  | total loss: \u001b[1m\u001b[32m0.24626\u001b[0m\u001b[0m | time: 3.332s\n",
      "| Optimizer | epoch: 015 | loss: 0.24626 - acc: 0.9725 | val_loss: 0.09437 - val_acc: 0.9721 -- iter: 55000/55000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# NAG\n",
    "tf.reset_default_graph()\n",
    "nag = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9, use_nesterov=True)\n",
    "net = tflearn.regression(get_layers(), optimizer=nag,\n",
    "                         loss='categorical_crossentropy')\n",
    "\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(X, Y, n_epoch=N_EPOCHS, validation_set=(testX, testY),\n",
    "          show_metric=True, run_id=\"nag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 12899  | total loss: \u001b[1m\u001b[32m0.30148\u001b[0m\u001b[0m | time: 2.286s\n",
      "| AdaGrad | epoch: 015 | loss: 0.30148 - acc: 0.9457 -- iter: 54976/55000\n",
      "Training Step: 12900  | total loss: \u001b[1m\u001b[32m0.28427\u001b[0m\u001b[0m | time: 3.358s\n",
      "| AdaGrad | epoch: 015 | loss: 0.28427 - acc: 0.9464 | val_loss: 0.18653 - val_acc: 0.9512 -- iter: 55000/55000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Adagrad\n",
    "tf.reset_default_graph()\n",
    "adagrad = tflearn.AdaGrad(learning_rate=0.01)\n",
    "net = tflearn.regression(get_layers(), optimizer=adagrad,\n",
    "                         loss='categorical_crossentropy')\n",
    "\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(X, Y, n_epoch=N_EPOCHS, validation_set=(testX, testY),\n",
    "          show_metric=True, run_id=\"adagrad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 12899  | total loss: \u001b[1m\u001b[32m0.30006\u001b[0m\u001b[0m | time: 2.543s\n",
      "| AdaDelta | epoch: 015 | loss: 0.30006 - acc: 0.9340 -- iter: 54976/55000\n",
      "Training Step: 12900  | total loss: \u001b[1m\u001b[32m0.28531\u001b[0m\u001b[0m | time: 3.613s\n",
      "| AdaDelta | epoch: 015 | loss: 0.28531 - acc: 0.9344 | val_loss: 0.15131 - val_acc: 0.9547 -- iter: 55000/55000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Adadelta\n",
    "tf.reset_default_graph()\n",
    "adadelta = tflearn.AdaDelta(learning_rate=1)\n",
    "net = tflearn.regression(get_layers(), optimizer=adadelta,\n",
    "                         loss='categorical_crossentropy')\n",
    "\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(X, Y, n_epoch=N_EPOCHS, validation_set=(testX, testY),\n",
    "          show_metric=True, run_id=\"adadelta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 12899  | total loss: \u001b[1m\u001b[32m0.23974\u001b[0m\u001b[0m | time: 2.408s\n",
      "| Adam | epoch: 015 | loss: 0.23974 - acc: 0.9285 -- iter: 54976/55000\n",
      "Training Step: 12900  | total loss: \u001b[1m\u001b[32m0.22898\u001b[0m\u001b[0m | time: 3.483s\n",
      "| Adam | epoch: 015 | loss: 0.22898 - acc: 0.9325 | val_loss: 0.28020 - val_acc: 0.9183 -- iter: 55000/55000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Adam\n",
    "tf.reset_default_graph()\n",
    "adam = tflearn.Adam(learning_rate=0.01)\n",
    "net = tflearn.regression(get_layers(), optimizer=adam,\n",
    "                         loss='categorical_crossentropy')\n",
    "\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(X, Y, n_epoch=N_EPOCHS, validation_set=(testX, testY),\n",
    "          show_metric=True, run_id=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 12899  | total loss: \u001b[1m\u001b[32m0.22663\u001b[0m\u001b[0m | time: 5.359s\n",
      "| Optimizer | epoch: 015 | loss: 0.22663 - acc: 0.9320 -- iter: 54976/55000\n",
      "Training Step: 12900  | total loss: \u001b[1m\u001b[32m0.22601\u001b[0m\u001b[0m | time: 6.432s\n",
      "| Optimizer | epoch: 015 | loss: 0.22601 - acc: 0.9310 | val_loss: 0.23752 - val_acc: 0.9321 -- iter: 55000/55000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Nadam\n",
    "tf.reset_default_graph()\n",
    "nadam = tf.contrib.opt.NadamOptimizer(learning_rate=0.01)\n",
    "net = tflearn.regression(get_layers(), optimizer=nadam,\n",
    "                         loss='categorical_crossentropy')\n",
    "\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(X, Y, n_epoch=N_EPOCHS, validation_set=(testX, testY),\n",
    "          show_metric=True, run_id=\"nadam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 12899  | total loss: \u001b[1m\u001b[32m0.35223\u001b[0m\u001b[0m | time: 2.857s\n",
      "| Optimizer | epoch: 015 | loss: 0.35223 - acc: 0.9201 -- iter: 54976/55000\n",
      "Training Step: 12900  | total loss: \u001b[1m\u001b[32m0.33797\u001b[0m\u001b[0m | time: 3.934s\n",
      "| Optimizer | epoch: 015 | loss: 0.33797 - acc: 0.9219 | val_loss: 0.17924 - val_acc: 0.9467 -- iter: 55000/55000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# AMSGrad\n",
    "tf.reset_default_graph()\n",
    "amsgrad = AMSGrad(learning_rate=0.01)\n",
    "net = tflearn.regression(get_layers(), optimizer=amsgrad,\n",
    "                         loss='categorical_crossentropy')\n",
    "\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(X, Y, n_epoch=N_EPOCHS, validation_set=(testX, testY),\n",
    "          show_metric=True, run_id=\"amsgrad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
